# How to Web Scrape with Python in 4 Minutes
https://towardsdatascience.com/how-to-web-scrape-with-python-in-4-minutes-bc49186a8460]

> "Web scraping is a technique to automatically access and extract large amounts of information from a website, which can save a huge amount of time and effort."

### Beautiful Soup docs

https://www.crummy.com/software/BeautifulSoup/bs4/doc/

### What is Web Scraping?

https://en.wikipedia.org/wiki/Web_scraping

# How to scrape websites without getting blocked

https://www.scrapehero.com/how-to-prevent-getting-blacklisted-while-scraping/

>"However, since most sites want to be on Google, arguably the largest scraper of websites globally, they do allow access to bots and spiders."

> "Web scraping bots fetch data very fast, but it is easy for a site to detect your scraper as humans cannot browse that fast. The faster you crawl, the worse it is for everyone. If a website gets too many requests than it can handle it might become unresponsive. Make your spider look real, by mimicking human actions. Put some random programmatic sleep calls in between requests, add some delays after crawling a small number of pages and choose the lowest number of concurrent requests possible. Ideally put a delay of 10-20 seconds between clicks and not put much load on the website, treating the website nice."

### Coversion tool
https://curl.trillworks.com/

### Selenium
https://www.selenium.dev/

### Pyppeteer
https://github.com/miyakogi/pyppeteer

> "Here are a few workarounds or tools which could help your headless browser-based scrapers from getting banned.
> 1. Puppeteer Extra  â€“ Puppeteer Stealth Plugin
> 2. Patching Selenium/ Phantom JS â€“ Stack OverFlow Answer on Patching Selenium with Chrome Driver
> 3. Fingerprint Rotation â€“ Microsoft Paper on Fingerprint Rotation"

> "How can websites detect and block web scraping?
how-do-websites-detect-web-scraping. Websites can use different mechanisms to detect a scraper/spider from a normal user. Some of these methods are enumerated below:
> 1. Unusual traffic/high download rate especially from a single client/or IP address within a short time span.
> 2. Repetitive tasks performed on the website in the same browsing pattern â€“ based on an assumption that a > human user wonâ€™t perform the same repetitive tasks all the time.
> 3. Checking if you are real browser â€“ A simple check is to try and execute javascript. Smarter tools can go a lot more and check your Graphic cards and CPUs ðŸ˜‰ to make sure you are coming from real browser.
> 4. Detection through honeypots â€“ these honeypots are usually links which arenâ€™t visible to a normal user but only to a spider. When a scraper/spider tries to access the link, the alarms are tripped."

### Web crawlering service

https://www.scrapehero.com/web-crawling-service/

# YouTube - Tracking Amazon Prices

https://www.youtube.com/watch?v=Bg9r_yLk7VY

# YouTube - Web Scraping in Python 101

https://www.youtube.com/watch?v=CHUxmVVH2AQ

[code401 Reading Notes](../401Python/code401Table.md)